{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot using RevDict and Llama 2 Chat Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/CE/julians/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/local/js/anaconda3/envs/main/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.17s/it]\n"
     ]
    }
   ],
   "source": [
    "id = 6\n",
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "access_token = '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'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=access_token, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=access_token, device_map={'':id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on GPU\n",
      "Model is on Cuda 0\n"
     ]
    }
   ],
   "source": [
    "# check current device\n",
    "if next(model.parameters()).is_cuda:\n",
    "    print(\"Model is on GPU\")\n",
    "    print(f\"Model is on Cuda {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"Model is on CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_prompt(message, chat_history, system_prompt):\n",
    "    ''' \n",
    "    This function creates the current prompt by considering the latest user input (message), the chat history and the system prompt\n",
    "    '''\n",
    "    texts = [f\"[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n \"]\n",
    "\n",
    "    for user_input, response in chat_history:\n",
    "        texts.append(f\"{user_input.strip()} [/INST] {response.strip()} </s><s> [INST] \")\n",
    "    \n",
    "    texts.append(f\"{message.strip()} [/INST]\")\n",
    "\n",
    "    current_prompt = \" \".join(texts)\n",
    "\n",
    "    return current_prompt\n",
    "\n",
    "def get_response(prompt):\n",
    "    ''' \n",
    "    This function returns the llama 2 response based on the current prompt.\n",
    "    '''\n",
    "    inputs = tokenizer([prompt], return_tensors='pt').to(\"cuda:6\")\n",
    "    output = model.generate(**inputs, max_new_tokens=20, temperature=.75, early_stopping=True, \n",
    "            )\n",
    "    chatbot_response = tokenizer.decode(output[:, inputs['input_ids'].shape[-1]:][0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # cut off last sentence if it doesn't terminate with a punctuation symbol \n",
    "    sentences = sent_tokenize(chatbot_response) \n",
    "    if len(sentences) > 1 and sentences[-1][-1] not in string.punctuation:\n",
    "        sentences.pop()\n",
    "    chatbot_response = ' '.join(sentences)\n",
    "\n",
    "    return chatbot_response\n",
    "\n",
    "def main(system_prompt):\n",
    "    '''\n",
    "    This function is called to initialize the conversation.\n",
    "    '''\n",
    "    history = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input()\n",
    "        print(f\"User: {user_input}\")\n",
    "        if user_input in [\"exit\", \"thank you\", \"bye\"]:\n",
    "            break\n",
    "\n",
    "        prompt = get_current_prompt(user_input, history, system_prompt)\n",
    "        response = get_response(prompt)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "        time.sleep(3)\n",
    "        history = history + [(user_input, response)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a system prompt\n",
    "# this is just a draft; the target 'instrument' will be replaced by the output of the reverse dictionary. Optionally, also the description is included  \n",
    "system_prompt = \"\"\"As an assistance chatbot, your task is to help the user find a word they are looking for.\n",
    "\n",
    "You know the word the user is looking for. The word is 'instrument'\n",
    "\n",
    "You help the user guess this word, so you are like a teacher. Never say this word to the user unless they guessed it. Always give short answers.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Great! Let me help you find the word you're thinking of.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example user input\n",
    "user_input = \"I need help to find the word.\"\n",
    "prompt = get_current_prompt(user_input, [], system_prompt)\n",
    "response = get_response(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: \n",
      "Chatbot: Great! I'm here to help you find the word you're thinking of.\n",
      "User: I need help to find the word.\n",
      "Chatbot: Sure, I'd be happy to help!\n",
      "User: Can you provide synonyms of my target?\n",
      "Chatbot: Of course! The word you are looking for is \"instrument\".\n",
      "User: exit\n"
     ]
    }
   ],
   "source": [
    "# start the conversation\n",
    "# NB: there is some bug within my loop that causes the output not the be printed immediately\n",
    "main(system_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
