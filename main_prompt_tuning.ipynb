{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/js/anaconda3/envs/main/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_model, PromptTuningConfig, PromptTuningInit, TaskType\n",
    "import torch\n",
    "import prompt_tuning\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of initalization tokens: 22\n",
      "Number of virtual tokens: 11\n",
      "PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='google/flan-t5-large', revision=None, task_type=<TaskType.SEQ_2_SEQ_LM: 'SEQ_2_SEQ_LM'>, inference_mode=False, num_virtual_tokens=11, token_dim=1024, num_transformer_submodules=2, num_attention_heads=16, num_layers=24, prompt_tuning_init=<PromptTuningInit.TEXT: 'TEXT'>, prompt_tuning_init_text='Given the following description, find the described target word. The target word is not contained in the description:', tokenizer_name_or_path='google/flan-t5-large')\n",
      "trainable params: 22,528 || all params: 783,172,608 || trainable%: 0.0028765050986052873\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "cuda_id = ':6'\n",
    "device = torch.device(f\"cuda{cuda_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# determine parameters for PromptTuning Configuration\n",
    "model_name = 'google/flan-t5-large'\n",
    "num_sub_modules = 2\n",
    "max_length = 128\n",
    "lr = 0.1\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) #, model_max_length=max_length)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "init_text = \"Given the following description, find the described target word. The target word is not contained in the description:\"\n",
    "\n",
    "# since the virtual tokens are shared across two transformers submodules the number of virtual tokens is computed by len(init_text_ids)/2\n",
    "init_token_ids = tokenizer(init_text)[\"input_ids\"]\n",
    "num_text_tokens = len(init_token_ids)\n",
    "print(f\"Number of initalization tokens: {num_text_tokens}\")\n",
    "\n",
    "num_virtual_tokens = num_text_tokens//num_sub_modules\n",
    "print(f\"Number of virtual tokens: {num_virtual_tokens}\")\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=num_virtual_tokens,\n",
    "    num_transformer_submodules=num_sub_modules,\n",
    "    inference_mode=False,\n",
    "    prompt_tuning_init_text=init_text,\n",
    "    tokenizer_name_or_path=model_name,\n",
    ")\n",
    "\n",
    "# create instance of PEFT-model\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "print(peft_model.active_peft_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of descriptions: 1000\n",
      "Number of target words: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 800/800 [00:00<00:00, 5809.20 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 100/100 [00:00<00:00, 4337.71 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 100/100 [00:00<00:00, 5612.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/local/js/BERT_Friends/data/coca_1000_final.csv\"\n",
    "\n",
    "train, eval, test, full_data = prompt_tuning.make_dataset(data_path)\n",
    "\n",
    "dataset_preproc = full_data.map(\n",
    "    prompt_tuning.preprocess_seq2seq,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=full_data['train'].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataloader, eval_dataloader, test_dataloader = prompt_tuning.make_dataloader(dataset_preproc, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_total, eval_preds_final = prompt_tuning.training(train_dataloader, eval_dataloader, peft_model, num_epochs, lr, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number for beam search and sequences to be returned\n",
    "num_beams = 5\n",
    "num_seqs = 5\n",
    "max_beam_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on GPU\n",
      "Top-k correct predictions: 72\n"
     ]
    }
   ],
   "source": [
    "# at inference for specific k\n",
    "\n",
    "# add argument 'file_spec=' with file path if you want to save the predicitons as a csv file\n",
    "# add argument 'zero-shot=True' if you want to get the predictions of FLAN-T5 large without being fine-tuned\n",
    "# add argument 'peft_model_id' when evaluating the prompt-tuned model with path to peft model bins\n",
    "\n",
    "peft_model_id = \"/local/js/prompt_tuning_bins/pt_20ep_01lr\"\n",
    "test_targets, test_preds = prompt_tuning.eval_seq2seq(test, test_dataloader, model.to('cpu'), peft_model_id=peft_model_id, num_beams=num_beams, num_seqs=num_seqs, device=device)\n",
    "acc = prompt_tuning.acc(test_targets, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-k prompt tuned\n",
    "top_k_acc_tuned = {}\n",
    "for i in range(max_beam_size):\n",
    "    _ , test_preds = prompt_tuning.eval_seq2seq(test, test_dataloader, model.to('cpu'), peft_model_id, i+1, i+1, device)\n",
    "    acc = prompt_tuning.acc(test['target'], test_preds)\n",
    "    top_k_acc_tuned[i+1] = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-k zero-shot\n",
    "top_k_acc_zs = {}\n",
    "for i in range(max_beam_size):\n",
    "    _ , test_preds = prompt_tuning.eval_seq2seq(test, test_dataloader, model.to('cpu'), i+1, i+1, device, zero_shot=True)\n",
    "    acc = prompt_tuning.acc(test['target'], test_preds)\n",
    "    top_k_acc_zs[i+1] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot zero-shot vs. prompt-tuned scores\n",
    "plt.plot(top_k_acc_tuned.keys(), top_k_acc_tuned.values(), label=\"Prompt Tuned\", color=\"blue\")\n",
    "plt.plot(top_k_acc_zs.keys(), top_k_acc_zs.values(), label=\"Zero Shot\", color=\"red\")\n",
    "plt.ylabel(\"top-k accuracy\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.title(\"k vs. top-k Accuracy FLAN-T5 large\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground for prompt-tuned model at inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:6'\n",
    "\n",
    "# make sure to replace the peft_model_id with the path to the folder that contains the peft model bins\n",
    "peft_model_id = \"/local/js/prompt_tuning_bins/pt_20ep_01lr\"\n",
    "\n",
    "# load peft model\n",
    "model, tokenizer = prompt_tuning.get_peft_model_inference(peft_model_id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generate(sentence, num_beams=10, num_seqs=10):\n",
    "    '''\n",
    "    This function generates targets for a given description using the previously initalized prompt-tuned model.\n",
    "    '''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if sentence[-1] != \".\":\n",
    "           sentence = sentence + \".\"\n",
    "        \n",
    "        inputs = [f\"Description : {sentence} Word : \"]\n",
    "        print(inputs)\n",
    "        inputs = tokenizer(inputs, return_tensors=\"pt\",)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'], max_new_tokens=10, num_beams=num_beams, num_return_sequences=num_seqs, early_stopping=True\n",
    "        )\n",
    "        \n",
    "        beam_preds = []\n",
    "        for beam_output in beam_outputs:\n",
    "            prediction = tokenizer.decode(beam_output.detach().cpu().numpy(), skip_special_tokens=True)\n",
    "            beam_preds.append(prediction)\n",
    "\n",
    "        return beam_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Description : a hairy thing growing on most of men on the upper side of their mouth. Word : ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['moustache',\n",
       " 'beard',\n",
       " 'mouton',\n",
       " 'shave',\n",
       " 'mouty',\n",
       " 'molar',\n",
       " 'upper lip',\n",
       " 'afro',\n",
       " 'mout',\n",
       " 'facial hair']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out model capabilities with any description\n",
    "description = 'a hairy thing growing on most of men on the upper side of their mouth'\n",
    "model_generate(description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
